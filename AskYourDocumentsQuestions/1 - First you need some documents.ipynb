{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 👉 First, you need some documents! 📑\n",
    "\n",
    "This notebook takes documents _(from whichever sources you specify)_ and loads\n",
    "them as LangChain `Document` objects, ready for processing in the next notebooks.\n",
    "\n",
    "Options available for loading documents:\n",
    "- **Local Files**: You can load documents from your local filesystem.\n",
    "- **LangChain Retrivers**: There are various built-in retrivers that can load\n",
    "  documents from the web, such as Wikipedia, etc.  \n",
    "  _This notebook includes a few examples: Wikipedia, Song Lyrics (from AZLyrics), Movie Scripts Database (from IMSDb)_\n",
    "\n",
    "> ℹ️ **Note**: This step is optional if you are exclusively using search indexes such as: Azure Cognitive Search, ElasticSearch, etc.  \n",
    "> _These manage their own document storage_\n",
    "\n",
    "> ℹ️ **Refer️ences**\n",
    "> - https://python.langchain.com/docs/integrations/retrievers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📂 Local Files\n",
    "\n",
    "The process of loading local files is:\n",
    "- 1. Specify path to folder with the desired documents\n",
    "- 2. Load each document using a [LangChain Document Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "\n",
    "The simplest way to load a document us using the [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file).\n",
    "\n",
    "This extracts the text from the desired document. By default, it extracts the entire text into a single string, but it can keep \"elements\" of the document together by using `mode=\"elements\"`.\n",
    "\n",
    "> _The goal of document partitioning is to read in a source document, split the document into sections, categorize those sections, and extract the text associated with those sections. Depending on the document type, unstructured uses different methods for partitioning a document._\n",
    "> \n",
    "> ~ https://unstructured-io.github.io/unstructured/getting_started.html\n",
    "\n",
    "If you want more fine-grained control over how documents are loaded, you browse and use specific [Document loaders](https://python.langchain.com/docs/integrations/document_loaders/), _e.g. you can find multiple ways of loading Word documents._\n",
    "\n",
    "For CSV/HTML/JSON/Markdown/PDF, you can find detailed information [in this other section](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n",
    "\n",
    "> ℹ️ **Refer️ences**\n",
    "> - https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/unstructured_file\n",
    "> - https://pypi.org/project/unstructured/\n",
    "> - https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "documents: list[Document] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder_with_documents = \"/path/to/folder/with/documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete or comment out this cell to prevent these example science documents from being loaded\n",
    "# Note: these documents are over 100MB and may take 10+ minutes to load\n",
    "path_to_folder_with_documents = \"../Resources/Example Documents/Science Translational Medicine\"\n",
    "glob_pattern = \"*.pdf\" # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you only want to load a single file (e.g. one of these large .pdf) then use this\n",
    "# Note: this single document is 35MB and takes approx. 1 minute to load\n",
    "path_to_single_file = \"../Resources/Example Documents/Science Translational Medicine/2020_Neuroscience.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading single document: ../Resources/Example Documents/Science Translational Medicine/2020_Neuroscience.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if path_to_single_file and os.path.isfile(path_to_single_file):\n",
    "    from langchain.document_loaders import UnstructuredFileLoader\n",
    "    print(f\"Loading single document: {path_to_single_file}\")\n",
    "    documents.extend(UnstructuredFileLoader(path_to_single_file).load())\n",
    "\n",
    "elif path_to_folder_with_documents and os.path.isdir(path_to_folder_with_documents):\n",
    "    from langchain.document_loaders import DirectoryLoader\n",
    "    print(f\"Loading all documents from folder: {path_to_folder_with_documents}\")\n",
    "    if glob_pattern:\n",
    "        documents.extend(DirectoryLoader(path_to_folder_with_documents, glob=\"*.pdf\", show_progress=True).load())\n",
    "    else:\n",
    "        documents.extend(DirectoryLoader(path_to_folder_with_documents, show_progress=True).load())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Files From The Interwebs\n",
    "\n",
    "The following cells load LangChain `Document`s from various places on the internet using various [Document loaders](https://python.langchain.com/docs/integrations/document_loaders/).\n",
    "\n",
    "_You can also make your own document loaders_.\n",
    "\n",
    "Other cool ones to call out are:\n",
    "- [Azure Blob Storage Container](https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_container) document loader\n",
    "- [Azure Blob Storage File](https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_file) document loader\n",
    "\n",
    "> ℹ️ **Refer️ences**\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/azlyrics\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/imsdb\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/youtube_transcript\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/wikipedia\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_container\n",
    "> - https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://www.azlyrics.com/lyrics/taylorswift/lovestory.html...\n",
      "Loading https://www.azlyrics.com/lyrics/alanismorissette/ironic.html...\n",
      "Loading https://www.azlyrics.com/lyrics/savestheday/thisisnotanexit.html...\n",
      "Loading https://www.azlyrics.com/lyrics/thrice/stareatthesun.html...\n"
     ]
    }
   ],
   "source": [
    "# This cell loads a few lyrics documents from AZLyrics.com\n",
    "from langchain.document_loaders import AZLyricsLoader\n",
    "\n",
    "song_urls = [\n",
    "    \"https://www.azlyrics.com/lyrics/taylorswift/lovestory.html\",\n",
    "    \"https://www.azlyrics.com/lyrics/alanismorissette/ironic.html\",\n",
    "    \"https://www.azlyrics.com/lyrics/savestheday/thisisnotanexit.html\",\n",
    "    \"https://www.azlyrics.com/lyrics/thrice/stareatthesun.html\",\n",
    "]\n",
    "\n",
    "for song_url in song_urls:\n",
    "    print(f\"Loading {song_url}\")\n",
    "    documents.extend(AZLyricsLoader(song_url).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://imsdb.com/scripts/Hackers.html\n",
      "Loading https://imsdb.com/scripts/Princess-Bride,-The.html\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the scripts from a couple of my favorite movies from IMSDb.com\n",
    "from langchain.document_loaders import IMSDbLoader\n",
    "\n",
    "movie_script_urls = [\n",
    "    \"https://imsdb.com/scripts/Hackers.html\",\n",
    "    \"https://imsdb.com/scripts/Princess-Bride,-The.html\"\n",
    "]\n",
    "\n",
    "for movie_script_url in movie_script_urls:\n",
    "    print(f\"Loading {movie_script_url}\")\n",
    "    documents.extend(IMSDbLoader(movie_script_url).load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://www.youtube.com/watch?v=dQw4w9WgXcQ\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the trascript from a YouTube music video\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "youtube_video_urls = [\n",
    "    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "]\n",
    "\n",
    "for youtube_video_url in youtube_video_urls:\n",
    "    print(f\"Loading {youtube_video_url}\")\n",
    "    documents.extend(YoutubeLoader.from_youtube_url(youtube_video_url).load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Large language model\n",
      "Searching for Generative AI\n"
     ]
    }
   ],
   "source": [
    "# This cell loads a few pages from Wikipedia\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "wikipedia_searches = [\n",
    "    \"Large language model\",\n",
    "    \"Generative AI\"\n",
    "]\n",
    "\n",
    "for search in wikipedia_searches:\n",
    "    print(f\"Searching for {search}\")\n",
    "    documents.extend(WikipediaLoader(search, load_max_docs=1).load())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 documents\n",
      " - Source: https://www.azlyrics.com/lyrics/taylorswift/lovestory.html\n",
      " - Source: https://www.azlyrics.com/lyrics/alanismorissette/ironic.html\n",
      " - Source: https://www.azlyrics.com/lyrics/savestheday/thisisnotanexit.html\n",
      " - Source: https://www.azlyrics.com/lyrics/thrice/stareatthesun.html\n",
      " - Source: https://imsdb.com/scripts/Hackers.html\n",
      " - Source: https://imsdb.com/scripts/Princess-Bride,-The.html\n",
      " - Source: dQw4w9WgXcQ\n",
      " - Source: https://en.wikipedia.org/wiki/Large_language_model\n",
      " - Source: https://en.wikipedia.org/wiki/Generative_artificial_intelligence\n",
      " - Source: ../Resources/Example Documents/Science Translational Medicine/2020_Neuroscience.pdf\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Documents _(for use in other notebooks)_\n",
    "\n",
    "Run the next cell to print out a summary of the documents you have stored.\n",
    "\n",
    "It will also `%store` the documents _(in your IPython database)_ for use in the next notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(documents)} documents\")\n",
    "for document in documents:\n",
    "    print(f\" - Source: {document.metadata['source']}\")\n",
    "\n",
    "print(f\"Storing {len(documents)} documents for the next notebooks...\")\n",
    "%store documents\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One you are ready, head over to [Notebook #2](#) 👉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-tOgwBZlw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

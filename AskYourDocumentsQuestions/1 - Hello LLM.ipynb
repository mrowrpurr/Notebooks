{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‹ Hello, LLM!\n",
    "\n",
    "Welcome to the _\"**Ask Your Documents Questions**\"_ tutorial notebook series!\n",
    "\n",
    "To kick things off, let's talk to an LLM, shall we?\n",
    "\n",
    "After all, the [Large Language Model](https://en.wikipedia.org/wiki/Large_language_model) is at the heart of all of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an LLM\n",
    "\n",
    "This notebook supports both running local LLMs as well as using hosted LLMs (_OpenAI ChatGPT_).\n",
    "\n",
    "When choosing an LLM, be sure to check the [licensing terms](https://huggingface.co/blog/os-llms#licensing). Some LLMs are only available for non-commercial use.\n",
    "\n",
    "For local models, this notebook uses Hugging Face to simplify the process of downloading and running the pretrained models (_both text generation [LLM] and sentence transformers [embeddings]_).\n",
    "\n",
    "Here are a fully open-source models which this notebook can run locally:\n",
    "- [MPT-7B](https://huggingface.co/mosaicml/mpt-7b) _(context length of 4,096 ~ there is a version with **65k tokens** requiring lots of GPU VRAM)_\n",
    "- [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) _(context length of 2,048 tokens)_\n",
    "\n",
    "The 7B models tend to be possible to run on customer GPU cards with ~16GB of memory. Your results may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLMs\n",
    "\n",
    "The following cells are your _\"Hello, world!\"_ cells for the local LLMs.\n",
    "\n",
    "You can skip these if so desired. They support nice GPUs to run.  \n",
    "(_This notebook tested on a Nvidia 2080 Ti_)\n",
    "\n",
    "> Note: these also require a fast network connection to download the models.\n",
    "> - `MPT-7B`: 12.3GB download _(note: it takes a bit longer to prepare the pipeline after the download)_\n",
    "> - `falcon-7b`: \n",
    "\n",
    "> Curious about `trust_remote_code=True`? See [here](https://huggingface.co/mosaicml/mpt-7b#how-to-use) and [here](https://www.reddit.com/r/LocalLLaMA/comments/13t2b67/security_psa_huggingface_models_are_code_not_just/jlu113p/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_MPT: bool = True\n",
    "run_FALCON: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell setups up MPT 7b and loads the model into GPU memory\n",
    "if run_MPT:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "    # Setup tokenizer\n",
    "    mpt_7b_tokenizer = AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b\", trust_remote_code=True)\n",
    "\n",
    "    # Setup model\n",
    "    mpt_7b_config = AutoConfig.from_pretrained(\"mosaicml/mpt-7b\", trust_remote_code=True)\n",
    "    # mpt_7b_config.max_seq_len = 4096 # TODO: Test if this slows down generatin\n",
    "    mpt_7b_config.init_device = \"cuda:0\" # For fast initialization directly on GPU!\n",
    "    mpt_7b_model = AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b\", trust_remote_code=True, config=mpt_7b_config)\n",
    "    \n",
    "    # Create a pipeline using the simpler pipeline interface\n",
    "    mpt_7b_pipeline = pipeline(\n",
    "        'text-generation',\n",
    "        model=mpt_7b_model,\n",
    "        tokenizer=mpt_7b_tokenizer,\n",
    "        device=\"cuda:0\", # If you're using Nvidia\n",
    "        trust_remote_code=True, # Required,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model can be repeated to test prompts for MPT 7b\n",
    "if run_MPT:\n",
    "    mpt_7b_prompt = \"What color is the sky on Mars?\"\n",
    "\n",
    "    # NOTE This takes 1.5 minutes on a 2080 Ti\n",
    "    mpt_7b_result = mpt_7b_pipeline(\n",
    "        mpt_7b_prompt,\n",
    "        max_new_tokens=50, # TODO: verify generation times with this set to different values\n",
    "        do_sample=True, # TODO: required or no?\n",
    "        use_cache=True # TODO: required or no?\n",
    "        # temperature=\n",
    "    )\n",
    "\n",
    "    # TODO: where to configure the temperature?\n",
    "    print(mpt_7b_result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Hosted GPT LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks-tOgwBZlw-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
